<!doctype html><html lang=en><head><link href=//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/css/bootstrap-combined.no-icons.min.css rel=stylesheet><link href=//netdna.bootstrapcdn.com/font-awesome/3.2.1/css/font-awesome.css rel=stylesheet><link href=/css/fontawesome-all.min.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fontawesome-all.min.css rel=stylesheet></noscript><link href=/css/featherlight.min.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/featherlight.min.css rel=stylesheet></noscript><link href=/css/nucleus.css rel=stylesheet><link href=/css/fonts.css rel=stylesheet media=print onload='this.media="all",this.onload=null'><noscript><link href=/css/fonts.css rel=stylesheet></noscript><link href=/css/theme.css rel=stylesheet><link href=/css/theme-relearn-light.css rel=stylesheet id=variant-style><link href=/css/print.css rel=stylesheet media=print><script src=/js/variant.js?1754062304></script>
<script>var root_url="/",baseUriFull,baseUri=root_url.replace(/\/$/,"");window.T_Copy_to_clipboard="",window.T_Copied_to_clipboard="",window.T_Copy_link_to_clipboard="",window.T_Link_copied_to_clipboard="",baseUriFull="http://localhost/",window.variants&&variants.init(["relearn-light"])</script><meta charset=utf-8><meta name=viewport content="height=device-height,width=device-width,initial-scale=1,minimum-scale=1"><meta name=generator content="Hugo 0.119.0"><meta itemprop=description property="description" content="Enable your GraphRAG pipeline to use private LLMs via Triton Inference Server"><meta property="og:url" content="http://localhost/3.13/data-science/graphrag/services/triton-inference-server/"><meta property="og:title" content="Triton LLM Host"><meta property="og:type" content="website"><meta property="og:description" content="Enable your GraphRAG pipeline to use private LLMs via Triton Inference Server"><meta name=docsearch:version content="3.13"><title>Triton LLM Host | ArangoDB Documentation</title><link href=/images/favicon.png rel=icon type=image/png><script src=/js/jquery.min.js></script>
<script src=/js/clipboard.min.js?1754062304 defer></script>
<script src=/js/featherlight.min.js?1754062304 defer></script>
<script>var versions=[{alias:"devel",deprecated:!1,name:"3.13",version:"3.13.0"},{alias:"stable",deprecated:!1,name:"3.12",version:"3.12.5"},{alias:"3.11",deprecated:!0,name:"3.11",version:"3.11.14"},{alias:"3.10",deprecated:!0,name:"3.10",version:"3.10.14"}]</script><script>var develVersion={alias:"devel",deprecated:!1,name:"3.13",version:"3.13.0"}</script><script>var stableVersion={alias:"stable",deprecated:!1,name:"3.12",version:"3.12.5"}</script><script src=/js/codeblocks.js?1754062304 defer></script>
<script src=/js/theme.js?1754062304 defer></script></head><body><noscript>You need to enable JavaScript to use the ArangoDB documentation.</noscript><div id=page-wrapper class=page_content_splash style=height:auto;opacity:0><section id=page-main><section class=page-container id=page-container><header id=header style="transition:.5s padding ease-out,.15s" class="zn_header_white header-splash-new nav-down header-splash-wrap header1"><div class=header-block-left><div class=mobile-menu-toggle><button id=sidebar-toggle-navigation onclick=showSidebarHandler()><svg width="1.33em" height="1.33em" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button></div><div class=version-logo-container><div class="logo-container hasinfocard_img arangodb-logo-large"><div class=logo><a href=https://www.arangodb.com/><img src=/images/logo_main.png alt=ArangoDB title></a></div></div><div class=arangodb-logo-small><a href=https://arangodb.com/><img alt="ArangoDB Logo" src=/images/ArangoDB_Logo_White_small.png></a></div></div></div><div class=container-right style=display:hidden></div><div class=search-and-version-container><a href=# class="home-link is-current" aria-label="Go to home page" onclick=goToHomepage(event)></a><div id=searchbox></div><script type=text/javascript>const SCRIPT_SRC="https://unpkg.com/@inkeep/widgets-embed@0.2.290/dist/embed.js";function loadAndInitializeInkeep(){if(document.querySelector(`script[src="${SCRIPT_SRC}]"`))return;const e=document.createElement("script");e.type="module",e.src=SCRIPT_SRC,e.onload=initializeInkeep,document.head.appendChild(e)}function initializeInkeep(){const e=Inkeep({integrationId:"clo4lx6jk0000s601cp21x2ok",apiKey:"13b4e56966a76e86c6ff359cd795ee6a0412f751d75d6383",organizationId:"org_HGBkkzGAa4KeGJGh",organizationDisplayName:"ArangoDB",primaryBrandColor:"#80a54d",stringReplacementRules:[{matchingRule:{ruleType:"Substring",string:"Arangograph"},replaceWith:"ArangoGraph"},{matchingRule:{ruleType:"Substring",string:"Aql"},replaceWith:"AQL"},{matchingRule:{ruleType:"Substring",string:"Arangodb"},replaceWith:"ArangoDB"}],customCardSettings:[{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"arango.qubitpi.org"}},searchTabLabel:"Official Docs"},{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"developer.arangodb.com"}},searchTabLabel:"Developer Hub"},{filters:{UrlMatch:{ruleType:"PartialUrl",partialUrl:"arangodb.com"}},searchTabLabel:"Home"}]}),t=e.embed({componentType:"ChatButton",properties:{stylesheetUrls:["/css/fonts.css"],fixedPositionXOffset:"52px",baseSettings:{theme:{primaryColors:{textColorOnPrimary:"white"},tokens:{fonts:{body:"'Inter'",heading:"'Inter'"},zIndex:{overlay:1e4,modal:11e3,popover:12e3,skipLink:13e3,toast:14e3,tooltip:15e3}}}},aiChatSettings:{botAvatarSrcUrl:"/images/ArangoDB_Logo_White_small.png",quickQuestions:["What can you do with AQL that is not feasible with SQL?","How do I search for objects within arrays?","Where can I deploy my ArangoDB instance?"],getHelpCallToActions:[{icon:{builtIn:"FaSlack"},name:"Slack",url:"https://arangodb-community.slack.com/"}]},searchSettings:{tabSettings:{isAllTabEnabled:!1,alwaysDisplayedTabs:["Official Docs","Developer Hub","Home"]}}}})}loadAndInitializeInkeep()</script><div class=version-selector><select id=arangodb-version onchange=changeVersion()><option value=3.13>3.13</option><option value=3.12>3.12</option><option value=3.11>3.11</option><option value=3.10>3.10</option><option value=3.9>3.9</option><option value=3.8>3.8</option></select></div></div></header><iframe src=/nav.html title=description id=menu-iframe class="menu-iframe active" style=opacity:0></iframe><div class=container-main><div class=row-main><nav id=breadcrumbs><ol class=links itemscope itemtype=http://schema.org/BreadcrumbList><meta itemprop=itemListOrder content="Descending"><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="3.13"><a itemprop=item class=link href=/3.13/><span itemprop=name class=breadcrumb-entry>3.13.0</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="GenAI & Data Science"><a itemprop=item class=link href=/3.13/data-science/><span itemprop=name class=breadcrumb-entry>GenAI & Data Science</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="GraphRAG"><a itemprop=item class=link href=/3.13/data-science/graphrag/><span itemprop=name class=breadcrumb-entry>GraphRAG</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Services"><a itemprop=item class=link href=/3.13/data-science/graphrag/services/><span itemprop=name class=breadcrumb-entry>Services</span></a>
<i class="fas fa-chevron-right fa-fw"></i></li><li itemscope itemtype=https://schema.org/ListItem itemprop=itemListElement><meta itemprop=position content="Triton LLM Host"><a itemprop=item class=link href=/3.13/data-science/graphrag/services/triton-inference-server/><span itemprop=name class=breadcrumb-entry>Triton LLM Host</span></a></li></ol></nav><article class=default><div class="box notices cstyle warning"><div class=box-content-container><div class=box-content><i class="fas fa-exclamation-triangle"></i><div class=box-text><p>ArangoDB v3.13 is under development and not released yet.
This documentation is not final and potentially incomplete.</p></div></div></div></div><hgroup><h1>Triton LLM Host</h1><p class=lead>Enable your GraphRAG pipeline to use private LLMs via Triton Inference Server</p></hgroup><p class=labels><span class=label>ArangoDB Platform</span></p><div class="box notices cstyle tip"><div class=box-content-container><div class=box-content><i class="fas fa-check"></i><div class=box-text>The ArangoDB Platform & GenAI Suite is available as a pre-release. To get
exclusive early access, <a href=https://arangodb.com/contact/ target=_blank rel="noopener noreferrer" class=link>get in touch</a>&nbsp;<i class="fas fa-external-link-alt"></i> with
the ArangoDB team.</div></div></div></div><h2 id=overview>Overview <a href=/3.13/data-science/graphrag/services/triton-inference-server/#overview class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The <strong>Triton LLM Host</strong> service provides scalable deployment of Large Language
Models (LLMs) using the NVIDIA Triton Inference Server. It efficiently serves
machine learning models with support for HTTP and gRPC APIs, customizable routing,
and seamless Kubernetes integration.</p><h2 id=workflow>Workflow <a href=/3.13/data-science/graphrag/services/triton-inference-server/#workflow class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The Triton LLM Host enables your GraphRAG pipeline to use privately hosted
LLMs directly from the ArangoDB Platform environment. The process involves the
following steps:</p><ol><li>Install the Triton LLM Host service.</li><li>Register your LLM model to MLflow by uploading the required files.</li><li>Configure the <a href=/3.13/data-science/graphrag/services/importer/#using-triton-inference-server-private-llm class=link>Importer</a> service to use your LLM model.</li><li>Configure the <a href=/3.13/data-science/graphrag/services/retriever/#using-triton-inference-server-private-llm class=link>Retriever</a> service to use your LLM model.</li></ol><div class="box notices cstyle tip"><div class=box-content-container><div class=box-content><i class="fas fa-check"></i><div class=box-text>Check out the dedicated <a href=/3.13/data-science/graphrag/services/mlflow/ class=link>ArangoDB MLflow</a> documentation page to learn
more about the service and how to interact with it.</div></div></div></div><h2 id=deployment>Deployment <a href=/3.13/data-science/graphrag/services/triton-inference-server/#deployment class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>The Triton LLM Host service is deployed as a <strong>Kubernetes application</strong> using Helm charts in
the ArangoDB Platform ecosystem. It integrates with the:</p><ul><li>MLFlow model registry for model management.</li><li>Storage sidecar for artifact storage.</li></ul><h2 id=installation-via-genai-service-api>Installation via GenAI Service API <a href=/3.13/data-science/graphrag/services/triton-inference-server/#installation-via-genai-service-api class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>To install the Triton LLM Host service, send an API request to the
<strong>GenAI service</strong> using the following parameters:</p><h3 id=required-parameters>Required parameters <a href=/3.13/data-science/graphrag/services/triton-inference-server/#required-parameters class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;models&#34;</span><span class=p>:</span> <span class=s2>&#34;model_name&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>}</span>
</span></span></code></pre></div><p>You can also specify multiple models:</p><ul><li>Without versions: <code>"model_name_1, model_name_2"</code></li><li>With versions: <code>"model_name_1@version1, model_name_2@version2"</code></li><li>Mixed: <code>"model_name_1, model_name_2@version4"</code></li></ul><h3 id=optional-parameters>Optional parameters <a href=/3.13/data-science/graphrag/services/triton-inference-server/#optional-parameters class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><div class=highlight><pre tabindex=0 class=chroma><code class=language-json data-lang=json><span class=line><span class=cl><span class=p>{</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;log_level&#34;</span><span class=p>:</span> <span class=s2>&#34;INFO&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>  <span class=nt>&#34;profiles&#34;</span><span class=p>:</span> <span class=s2>&#34;profile1,profile2&#34;</span>
</span></span><span class=line><span class=cl>  <span class=s2>&#34;resources_requests_memory&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span>     <span class=c1>// Minimum memory required for the container
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=nt>&#34;resources_requests_cpu&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span>        <span class=c1>// Minimum CPU required for the container
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=nt>&#34;resources_limits_memory&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span>       <span class=c1>// Maximum memory the container can use
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=nt>&#34;resources_limits_cpu&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span>          <span class=c1>// Maximum CPU the container can use
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=nt>&#34;resources_requests_ephemeral_storage&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span><span class=p>,</span>  <span class=c1>// Minimum ephemeral storage required for the container
</span></span></span><span class=line><span class=cl><span class=c1></span>  <span class=nt>&#34;resources_limits_ephemeral_storage&#34;</span><span class=p>:</span> <span class=s2>&#34;&#34;</span>     <span class=c1>// Maximum ephemeral storage the container can use
</span></span></span><span class=line><span class=cl><span class=c1></span><span class=p>}</span>
</span></span></code></pre></div><h3 id=parameter-descriptions>Parameter descriptions <a href=/3.13/data-science/graphrag/services/triton-inference-server/#parameter-descriptions class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><table><thead><tr><th>Parameter</th><th>Required</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td><code>models</code></td><td>✅</td><td>Comma-separated list of model_name@version pairs</td><td><code>"mistral@1,t5@3"</code></td></tr><tr><td><code>resources_requests_memory</code></td><td>❌</td><td>Minimum memory required</td><td><code>"8Gi"</code></td></tr><tr><td><code>resources_requests_cpu</code></td><td>❌</td><td>Minimum CPU cores required</td><td><code>"2"</code></td></tr><tr><td><code>resources_limits_memory</code></td><td>❌</td><td>Maximum memory allowed</td><td><code>"16Gi"</code></td></tr><tr><td><code>resources_limits_cpu</code></td><td>❌</td><td>Maximum CPU cores allowed</td><td><code>"4"</code></td></tr><tr><td><code>log_level</code></td><td>❌</td><td>Logging level</td><td><code>"INFO"</code> (default)</td></tr><tr><td><code>profiles</code></td><td>❌</td><td>Platform profiles to apply</td><td><code>"gpu,performance"</code></td></tr></tbody></table><h2 id=model-requirements>Model requirements <a href=/3.13/data-science/graphrag/services/triton-inference-server/#model-requirements class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><h3 id=python-backend>Python Backend <a href=/3.13/data-science/graphrag/services/triton-inference-server/#python-backend class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>All models <strong>must use the Python backend</strong> to ensure compatibility with the
Triton service. Each model requires the following two files:</p><ol><li><p><strong><code>model.py</code></strong>
Implements the Python backend model. Triton uses this file to load and
execute your model for inference.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>class</span> <span class=nc>TritonPythonModel</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>initialize</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>args</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Load your model here</span>
</span></span><span class=line><span class=cl>        <span class=k>pass</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>execute</span><span class=p>(</span><span class=bp>self</span><span class=p>,</span> <span class=n>requests</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Process inference requests</span>
</span></span><span class=line><span class=cl>        <span class=k>pass</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>def</span> <span class=nf>finalize</span><span class=p>(</span><span class=bp>self</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=c1># Cleanup resources</span>
</span></span><span class=line><span class=cl>        <span class=k>pass</span>
</span></span></code></pre></div></li><li><p><strong><code>config.pbtxt</code></strong>
This is the Triton model configuration file that defines essential parameters
such as the model name, backend, and input/output tensors.</p><pre tabindex=0><code>name: &#34;your_model_name&#34;
backend: &#34;python&#34;
max_batch_size: 1
input: [...]
output: [...]
</code></pre></li></ol><h2 id=model-management-with-mlflow>Model management with MLflow <a href=/3.13/data-science/graphrag/services/triton-inference-server/#model-management-with-mlflow class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><div class="box notices cstyle info"><div class=box-content-container><div class=box-content><i class="fas fa-info-circle"></i><div class=box-text>To prepare your Python backend model for the Triton LLM Host, you must first
register it in MLflow. The Triton LLM Host service automatically downloads
and load models from the MLflow registry.</div></div></div></div><h3 id=how-to-register-a-model-in-mlflow>How to register a model in MLflow <a href=/3.13/data-science/graphrag/services/triton-inference-server/#how-to-register-a-model-in-mlflow class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h3><p>Registering a Python backend model in MLflow involves packaging your
<code>model.py</code> and <code>config.pbtxt</code> files and passing them as an artifact. The Triton
service will look for a directory named after your model (e.g., <code>my-private-llm-model</code>)
within the MLflow registry store and expects to find the <code>model.py</code> and <code>config.pbtxt</code>
files inside it.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-py data-lang=py><span class=line><span class=cl><span class=k>try</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=n>mlflow</span><span class=o>.</span><span class=n>set_tracking_uri</span><span class=p>(</span><span class=n>MLFLOW_SERVICE_URI</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>with</span> <span class=n>mlflow</span><span class=o>.</span><span class=n>start_run</span><span class=p>()</span> <span class=k>as</span> <span class=n>run</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>run_id</span> <span class=o>=</span> <span class=n>run</span><span class=o>.</span><span class=n>info</span><span class=o>.</span><span class=n>run_id</span>
</span></span><span class=line><span class=cl>        <span class=n>model_uri</span> <span class=o>=</span> <span class=sa>f</span><span class=s2>&#34;runs:/</span><span class=si>{</span><span class=n>run_id</span><span class=si>}</span><span class=s2>/model&#34;</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow</span><span class=o>.</span><span class=n>register_model</span><span class=p>(</span><span class=n>model_uri</span><span class=o>=</span><span class=n>model_uri</span><span class=p>,</span> <span class=n>name</span><span class=o>=</span><span class=n>model_name</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=c1># Log the entire model directory as an artifact, preserving the Triton structure</span>
</span></span><span class=line><span class=cl>        <span class=n>mlflow</span><span class=o>.</span><span class=n>log_artifact</span><span class=p>(</span><span class=n>local_path</span><span class=o>=</span><span class=nb>str</span><span class=p>(</span><span class=n>local_model_dir</span><span class=p>))</span>
</span></span></code></pre></div><h2 id=service-endpoints>Service endpoints <a href=/3.13/data-science/graphrag/services/triton-inference-server/#service-endpoints class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>Once deployed, the service exposes two endpoints:</p><table><thead><tr><th>Port</th><th>Protocol</th><th>Purpose</th></tr></thead><tbody><tr><td>8000</td><td>HTTP/REST</td><td>Model inference, management, status</td></tr><tr><td>8001</td><td>gRPC</td><td>High-performance binary communication</td></tr></tbody></table><div class="box notices cstyle info"><div class=box-content-container><div class=box-content><i class="fas fa-info-circle"></i><div class=box-text>The Triton Inference Server is not intended to be used in a standalone mode.
Instead, other services consume these endpoints to send inference
requests for example. Refer to the specific service with which you are using
Triton Inference Server for more details.</div></div></div></div><ul><li><p><strong>Internal access (within ArangoDB Platform)</strong>:
<code>https://{SERVICE_ID}.{KUBERNETES_NAMESPACE}.svc:8000</code></p><ul><li><code>KUBERNETES_NAMESPACE</code> is available as an environment variable.</li><li><code>SERVICE_ID</code> is returned by the GenAI service API.</li></ul><p><strong>Example</strong>:
To check server health:
<code>GET https://{SERVICE_ID}.{KUBERNETES_NAMESPACE}.svc:8000/v2/health/ready</code></p></li><li><p><strong>External access (outside ArangoDB Platform)</strong>:
<code>https://{BASE_URL}:8529/llm/{SERVICE_POSTFIX}/</code></p><ul><li><code>BASE_URL</code>: Your ArangoDB Platform base URL.</li><li><code>SERVICE_POSTFIX</code>: Last 5 characters of the service ID.</li></ul><p><strong>Example</strong>:
To check server health:
<code>GET https://{BASE_URL}:8529/llm/{SERVICE_POSTFIX}/v2/health/ready</code></p></li></ul><div class="box notices cstyle info"><div class=box-content-container><div class=box-content><i class="fas fa-info-circle"></i><div class=box-text>Only HTTP protocol is supported for external access (outside the ArangoDB
Platform). For gRPC, use internal endpoints. This limitation applies to model
inference, model management, model status, and health check endpoints.</div></div></div></div><h2 id=triton-inference-server-api>Triton Inference Server API <a href=/3.13/data-science/graphrag/services/triton-inference-server/#triton-inference-server-api class=header-link onclick=copyURI(event)><span></span><i class="fa fa-link"></i></a></h2><p>For complete documentation on available endpoints and their usage,
refer to the <a href=https://docs.nvidia.com/deeplearning/triton-inference-server/archives/triton_inference_server_1120/triton-inference-server-guide/docs/http_grpc_api.htm target=_blank rel="noopener noreferrer" class=link>Triton Inference Server HTTP API</a>&nbsp;<i class="fas fa-external-link-alt"></i> documentation.</p><nav class=pagination><span class=prev><a class="nav nav-prev link" href=/3.13/data-science/graphrag/services/mlflow/><i class="fas fa-chevron-left fa-fw"></i><p>MLflow</p></a></span><span class=next></span></nav></article><div class=toc-container><a class=edit-page aria-label href=https://github.com/arangodb/docs-hugo/edit/main/site/content/3.13/data-science/graphrag/services/triton-inference-server.md target=_blank><i class="fab fa-fw fa-github edit-page-icon"></i></a><div class=toc><div class=toc-content><div class=toc-header><p>On this page</p></div><nav id=TableOfContents><div class=level-2><a href=#overview>Overview</a></div><div class=level-2><a href=#workflow>Workflow</a></div><div class=level-2><a href=#deployment>Deployment</a></div><div class=level-2><a href=#installation-via-genai-service-api>Installation via GenAI Service API</a></div><div class=level-3><a href=#required-parameters>Required parameters</a></div><div class=level-3><a href=#optional-parameters>Optional parameters</a></div><div class=level-3><a href=#parameter-descriptions>Parameter descriptions</a></div><div class=level-2><a href=#model-requirements>Model requirements</a></div><div class=level-3><a href=#python-backend>Python Backend</a></div><div class=level-2><a href=#model-management-with-mlflow>Model management with MLflow</a></div><div class=level-3><a href=#how-to-register-a-model-in-mlflow>How to register a model in MLflow</a></div><div class=level-2><a href=#service-endpoints>Service endpoints</a></div><div class=level-2><a href=#triton-inference-server-api>Triton Inference Server API</a></div></nav></div></div></div></div></div></section></section></div><button class="back-to-top hidden" onclick=goToTop(event) href=#><i class="fa fa-arrow-up"></i></button><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/@docsearch/css@3>
<script src=https://cdn.jsdelivr.net/npm/@docsearch/js@3></script>
<script type=text/javascript>window.setupDocSearch=function(e){if(!window.docsearch)return;docsearch({appId:"OK3ZBQ5982",apiKey:"500c85ccecb335d507fe4449aed12e1d",indexName:"arangodbdocs",insights:!0,container:"#searchbox",debug:!1,maxResultsPerGroup:10,searchParameters:{facetFilters:[`version:${e}`]}})}</script></body></html>